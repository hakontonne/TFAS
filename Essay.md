# Essay 





## Introduction 

Humans  seem  to  have  the  best  of  two  worlds;  fast,  instinctive  reactions  anddeep  logical  thinking  done  over  long  time.   We  can  catch  a  thrown  ball  withonly a few hundred milliseconds (Kenward and Nilsson 2011) to react.  We readeach others faces and predict emotions, judge intent by the sound of a voice, weride bikes and even drive cars on instinct, without much mental effort.  At thesame time, we have the ability sit down and do complex mathematics, constructintricate logical arguments or write complicated fictional stories with, requiringmuch mental effort, and in hindsight we fret over our social awkwardness andour shortcomings.  In busy or crowded places, we can easily focus our attentionon some arbitrary detail, like a familiar sound, a particular hair colour or ourown car, while discarding otherwise attention-grabbing impulses.This is the foundation of psychology theories within cognitive control, wherehumans have two modes of thought, System 1 and System 2, or fast and slow,one  instinctive  and  one  rational.   Above,  some  examples  of  the  two  types  ofsystem  are  given.   Instinctively,  the  slow  rational  part  of  us  is  viewed  as  thegreater of these two systems, but 95 % of a normal day, a person relies solely onthe fast system, while the slow system is running in the background, using littleeffort.  The strength of the fast system comes from its ability to be trained bythe slow system, it can learn more complex tasks as we grow and take over newtasks.  For example, many exprience ”driving on autopilot”, not rememberingthe  drive  to  work  or  beeing  completly  lost  in  thought  while  driving.   This  isthe fast system at work, where the drive took low mental effort and the act ofdriving the car was not in the forefront of the mind, but this must have beenlearned.  To begin with, driving was a demanding task and required a great dealof  attention.   Here,  the  slow  system  was  involved,  constantly  monitioring  thefast system, correcting and coaching.  Grandmasters in chess can rely on theirfast system to judge the state of a chessboard almost instantly, but no one isborn with this ability.Thus it is quite apparent that the fast system must be trainable by the slowermore rational system.  Kahneman offers a good overview of these two systemsand their interaction between each other:

> System 1 (fast system) runs automatically and System 2 is normallyin  a  comfortable  low-effort  mode,  in  which  only  a  fraction  of  itscapacity  is  engaged.   System  1  continuously  generates  suggestionsfor System 2 (slow system):  impressions, intuitions, intentions andfeelings.  If endorsed by System 2, impressions and feelings turn intobeliefs,  and  impulses  turn  into  voluntary  actions.   When  all  goessmoothly, which is most of the time, System 2 adopts the suggestionof System 1 with little or no modification.  (.  .  .  )  When System 1runs into difficulty, it calls on System 2 to support more detailed andspecific processing that may solve the problem of the moment.  (.  .  .)  System 2 is activated when an event is detected that violates themodel of the world System 1 maintains, or when System 2 detects an error about to be made



This  division  of  labour  is  highly  efficient,  as  the  fast  system  is  sufficientfor the majority of tasks encounter, it performs well in familiar situations and1

provides good short-term predictions, but the slow system is imperative, withoutit, humans would be inflexible, solely reactive, unable to learn new skills to adaptwell to new environments and situations.Since the beginning of the 2010s machine learning and deep learning havecontinued to prove its unparalleled performance in a wide range of applications,but the algorithms developed are only able to retain high performance in narrowfields.  A trained artificial neural network that is applied to a new task, is proneto ”catastrophic forgetting”, where the network loses its performance in previ-ously mastered tasks in favour of new tasks.  Therefore,  for each application,a new algorithm must be trained with an accompanying dataset, which can bevery data inefficient.  The concept of meta-learning, ”learning to learn”, tries toavoid this by learning a general structure for all the tasks it will be exposed to.Another approach could be to retain some inherent information about each taskthe algorithm is exposed to, so when the task is mastered, the networks statecan be saved to a knowledge database.  When this task is later encountered, thestate of the network can used to solve the task quickly.  Another approach, veryrecently  proposed  within  the  field  of  reinforcement  learning,  is  to  move  awayfrom model-free algorithms.  In addition to learning the tasks, the network triesto learn a model about the work, then use this model as well to solve the task.When  a  new  task  is  encountered,  the  network  still  retains  information  aboutthe world and its inherent properties (such as gravity, inertia and such), giventhat the task is within the same domain.  This does not address the problem ofcatastrophic forgetting, but alleviates the problem by exploiting the fact thatthe world doesn’t change and is unnecessary to relearn.In this project, we attempt to combine the use of consolidating knowledgeinto predispositions and model-based algorithms, into a model that models thefast and slow systems described earlier.



## Relevant methods



### Machine learning

Heuristic algorithms use rules to solve a task.  For example given an input, if thisinput is above a certain threshold, apply this operation and return the result.Machine  learning  algorithms  differ  from  heuristic  methods  by  not  using  therules to solve a task, but attempts to learn to solve the task.  The three types ofmachine learning differs in how they learn; in supervised learning the algorithmtrains on a labeled dataset where the information we want is already known.In unsupervised learning the algorithm tries to find patterns and connectionsin large unlabeled datasets, modeling probability densities over the inputs.  Inreinforcement learning, we don’t have a labeled ground truth, but we can inferhow good a prediction is, usually by a reward function.



## Previous work

Attempts at implementing the division of labour modeled have been done, themost relevant from Tzuu-Hseng S. Li et.  al., where in 2016, a proof-of-conceptwas shown (Li et al.  2016).  In this paper, they demonstrated a humanoid robotwho’s task was to throw a ball into a basket, using the model of two systems.2

System 1 was set two be two polynomials, one for rotational speed and one forthe angles of the arm used to throw the ball.  These polynomials where usedto fit experience curves for each polynomial, based on distance and angle fromthe robot to the basket.  The authors used 3 different methods for System 2,short-term  memory  (only  the  last  shooting  attempted  is  remembered),  long-term  memory  (all  attempts  is  are  remembered)  and  peek-end  rule  (the  best,last  attempts  is  remembered),  another  theory  proposed  by  Kahneman.   Theremembered attempts is are then used with a given correction rule to improvethe coefficients of the two polynomials determining rotational speed and angle.This setup showed promising results, cutting down the learning time from 90minutes to 20 minutes based on this setup, they especially note the usefulnessof  anchors  and  peak-end  rule.   Later,  in  2019  the  authors  published  a  newpaper  (Li  et  al.   2019),  this  time  proposing  a  Deep  Belief  network  (DBN).The DBN is a simple neural network of stacked restricted Boltzmann machines,used to suggest angles for the entire arm and velocity of the joint between theoverarm  and  the  shoulder.   This  replaces  the  two  polynomials  as  System  1,while  a  population-based  intelligent  optimization  (PSO)  algorithm  is  used  asSystem 2.  PSO originates from swarm intelligence and uses particles to searchfor the best solutions in D-dimensional space.  Here, it is modified to also useinertia  weighting  to  for  balancing  both  the  local  and  global  search  space,  toavoid getting stuck in local maxima.  The two networks is are then trained onalternating examples, until the DBN achieves a set accuracy level.  This papershows some faster convergence with DBNs compared to ANN (unspecified bythe paper), while still achieving 100 % accuracy.  The PSO was not very muchused, it only intervened 5 times throughout the training process.[?] showed a novel method for mitigating the catastrophic forgetting prob-lem.  They moved away from model-free algorithms,  where the goal is to usebrute-force  to  find  solutions,  building  a  policy  network  that  react  to  the  in-puts with no inherent understanding of the world.  Planning based algorithms,like  PlaNet,  learns  stochastic  and  deterministic  kinematics  of  the  world,  cre-ating a model of the world and use this to plan ahead.  This planning allowsthe  algorithm  to  find  better  solutions  with  far  less  data.   Experiments  doneby [@planet] find that it achieves similar performance as leading reinforcementlearning algorithm with upto 100 times less episodes.The team trained the network on six different tasks within the same domain(OpenAI  Gym),  where  the  network  had  already  learned  rudimentary  under-standing of gravity and dynamics from the first task.  This knowledge was thenreused when learning the new tasks to achieve high performance with, on aver-age, 50 times more efficiency than comparable algorithms.Additionally, [@planet] used encoders to create compact latent states fromimages,  instead of using the images directly.  Due to its more compact statesthan images, the network is able plan/predict longer.Peter  Norstein,  in  his  master  thesis,  created  a  novel  system  modeling  thefast and slow system with reinforcement learning.



## Original ideas





While some progress has been made in this field, the implementations have allbeen  very  restrained;  some  things  are  already  known  about  the  problem  (we3

Figure 1:  Comparision of different algorithms and methods used in PlaNet, in6 different taskts in the same domainneed to approximate two polynomials or we need a neural network to predictthese 4 variables), they rely heavily on highly accurate sensors and due to thisisn’t directly transferable to other fields.  If you would want to implement any ofthese described methods, it would require a lot of adjusting and customizationto make the implementation fit the problem before us.  We need to be awareof  what  this  method  seeks  to  improve,  what  is  its  main  advantage?   In  thismachine learning setting, its cutting heavily down on the compute cost of moreadvanced models, by learning or creating a simpler model that works for 90 %of the time and can be further improved by the more advanced model.  Thisdoes not restrain us to use polynomials or using a set of known variables for thespecific problem in question, but new advances withinReinforcement Learning and Deep Neural Networks allows us to create sys-tem that is agnostic to its environments and, more recently, to its own body/actor.If we were able to train an advanced Deep Neural Network, perhaps a Reinforce-ment Learning setup, and then simplify or approximate this system while stillreaching  90  %  accuracy  of  the  full  system  and  have  the  ability  to  be  trainedfurther, we could implement this as a dual system comprising of two neural net-works of different complexity.  Preferably we would want to infer the complexmodel (System 2) after we infer the simpler model (System 1), but perhaps ourmain contribution can come in this area.  If we can create a system that usesSystem 1’s confusion matrix, as well as some probability, to infer System 2 aswell,  we  could  even  have  a  small  additional  neural  network  that  learns  whenSystem 1’s suggestion should be verified by System 2, so that we can start toinfer System 2 while we also infer System 1 and then wait for the System 2’sprediction.  This could speed up cases when we need to infer System 2 as well.I would like to explore this area by using architectures that are known to beperformant , but expensive and see if one can derive a simpler model that canact as System 1.  If this is successful, then we can explore the logic and controlsfor switching and infering between these two systems.  I propose to first explore4

the possibility to simplyfy?  artificial networks and their structure drasticallywhile maintaining an accuracy/precision well within reason, 90-70 %, but cut-ting down on infer times drastically.  After this, I propose to use some kind ofsimulator or video game and create an artificial neural network, like an autopilotfor a car simulator, to then see if this duality of models has an advantage andif we can create some smart system for predicting a hard case for System 1. 

